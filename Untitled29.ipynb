{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObh7vgTXPZaAkf5JTcc3OJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2503a54l01-alt/NLP/blob/main/Untitled29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFfpZPsf4XvX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71179f12"
      },
      "source": [
        "# Task\n",
        "Install NLTK and spaCy, load a text paragraph, count the words, convert the text to lowercase, and then summarize the word count and the lowercase version of the text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b644b83"
      },
      "source": [
        "## Install and Import NLP Libraries\n",
        "\n",
        "### Subtask:\n",
        "Install NLTK and spaCy libraries and import them, along with downloading necessary NLTK data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9209a86b"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin, I'll install the required NLP libraries, NLTK and spaCy, and then proceed to import them. Afterward, I'll download the 'punkt' tokenizer from NLTK, which is crucial for various text processing tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ccdef7d",
        "outputId": "b330710b-d807-427f-dceb-d86f371f4bef"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk spacy\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"NLTK and spaCy installed, imported, and NLTK 'punkt' data downloaded successfully.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK and spaCy installed, imported, and NLTK 'punkt' data downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9628f2a"
      },
      "source": [
        "**Reasoning**:\n",
        "Since NLTK and spaCy are successfully installed, the next logical step is to load the text paragraph that will be processed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8103dace",
        "outputId": "26c5c18d-cb40-4639-ee40-65bc9e511b0a"
      },
      "source": [
        "## Load Text Paragraph\n",
        "\n",
        "### Subtask:\n",
        "Load a sample text paragraph for processing.\n",
        "\n",
        "#### Instructions\n",
        "1. Define a multiline string variable named `text_paragraph` containing the text to be processed. This paragraph will be used for word counting and case conversion.\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-725226151.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-725226151.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Load a sample text paragraph for processing.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51fdd128"
      },
      "source": [
        "## Tokenize Text Paragraph\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the `text_paragraph` using NLTK's `word_tokenize` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "155ba934",
        "outputId": "7ee10067-ebda-4259-9ed0-88f42c7f8b5d"
      },
      "source": [
        "tokenized_text = nltk.word_tokenize(text_paragraph)\n",
        "\n",
        "print(\"Tokenized text:\")\n",
        "print(tokenized_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', ',', 'machine', 'learning', ',', 'and', 'computational', 'linguistics', '.', 'It', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', ',', 'process', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'combines', 'various', 'techniques', ',', 'including', 'statistical', 'methods', ',', 'rule-based', 'approaches', ',', 'and', 'more', 'recently', ',', 'deep', 'learning', ',', 'to', 'bridge', 'the', 'gap', 'between', 'human', 'communication', 'and', 'computer', 'understanding', '.', 'Its', 'applications', 'range', 'from', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'to', 'chatbots', 'and', 'speech', 'recognition', '.', 'The', 'field', 'continues', 'to', 'evolve', 'rapidly', 'with', 'advancements', 'in', 'neural', 'networks', 'and', 'large', 'language', 'models', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c076ea6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block resulted in a `SyntaxError` because it contained markdown text instead of valid Python code. The goal is to define a multiline string variable for the text paragraph. I will correct the `code` field to contain only valid Python syntax that defines the `text_paragraph` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d1b3ddb",
        "outputId": "3e1d73f3-f0fc-4fb0-cb5f-fd2a19191022"
      },
      "source": [
        "text_paragraph = \"\"\"Natural Language Processing (NLP) is a subfield of artificial intelligence, machine learning, and computational linguistics. It focuses on enabling computers to understand, process, and generate human language. NLP combines various techniques, including statistical methods, rule-based approaches, and more recently, deep learning, to bridge the gap between human communication and computer understanding. Its applications range from sentiment analysis and machine translation to chatbots and speech recognition. The field continues to evolve rapidly with advancements in neural networks and large language models.\"\"\"\n",
        "\n",
        "print(\"Text paragraph loaded successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text paragraph loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "806ecc42"
      },
      "source": [
        "## Process Text: Word Count and Lowercase Conversion\n",
        "\n",
        "### Subtask:\n",
        "Count the total number of words in the loaded text paragraph and convert the entire paragraph to lowercase.\n",
        "\n",
        "#### Instructions\n",
        "1. Count the number of words in `text_paragraph` using NLTK's `word_tokenize` and store it in a variable named `word_count`.\n",
        "2. Convert the `text_paragraph` to its lowercase equivalent and store it in a variable named `lowercase_text`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "127f87a8"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the definition of the subtask, I will now write the Python code to count the words using NLTK's `word_tokenize` function and convert the `text_paragraph` to lowercase, storing the results in the specified variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edf4b7d4",
        "outputId": "607e9dc6-231b-458b-8599-41dde670b8ec"
      },
      "source": [
        "word_count = len(nltk.word_tokenize(text_paragraph))\n",
        "lowercase_text = text_paragraph.lower()\n",
        "\n",
        "print(f\"Word count completed: {word_count} words\")\n",
        "print(\"Text converted to lowercase.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count completed: 96 words\n",
            "Text converted to lowercase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b5ee37"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `punkt_tab` NLTK resource, required by `word_tokenize` for some tokenization tasks, was not downloaded. I will add a new code cell to download this specific resource, as suggested by the error message, and then proceed with the word counting and lowercase conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c6e387f",
        "outputId": "9f109d3e-5af7-45ff-c2a0-f3430c47892b"
      },
      "source": [
        "nltk.download('punkt_tab')\n",
        "\n",
        "word_count = len(nltk.word_tokenize(text_paragraph))\n",
        "lowercase_text = text_paragraph.lower()\n",
        "\n",
        "print(f\"Word count completed: {word_count} words\")\n",
        "print(\"Text converted to lowercase.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word count completed: 96 words\n",
            "Text converted to lowercase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0926e932"
      },
      "source": [
        "**Reasoning**:\n",
        "With the word count and lowercase text successfully generated, the final step is to summarize and display these results to complete the task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20d4cfce",
        "outputId": "e90805d9-6be2-4fdb-8574-1233192de388"
      },
      "source": [
        "print('--- Summary ---')\n",
        "print(f\"Total word count: {word_count}\")\n",
        "print(f\"Lowercase text:\\n{lowercase_text}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Summary ---\n",
            "Total word count: 96\n",
            "Lowercase text:\n",
            "natural language processing (nlp) is a subfield of artificial intelligence, machine learning, and computational linguistics. it focuses on enabling computers to understand, process, and generate human language. nlp combines various techniques, including statistical methods, rule-based approaches, and more recently, deep learning, to bridge the gap between human communication and computer understanding. its applications range from sentiment analysis and machine translation to chatbots and speech recognition. the field continues to evolve rapidly with advancements in neural networks and large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7a6e7c1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The NLTK and spaCy libraries were confirmed to be installed in the environment, and the `punkt` tokenizer for NLTK was successfully downloaded.\n",
        "*   A text paragraph was loaded, serving as the input for the NLP operations.\n",
        "*   An initial attempt to count words using `nltk.word_tokenize` failed due to a missing `punkt_tab` resource, which was subsequently downloaded to resolve the issue.\n",
        "*   After resolving the missing resource dependency, the total word count for the loaded paragraph was determined to be 96.\n",
        "*   The entire text paragraph was successfully converted to its lowercase equivalent.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Ensure all necessary NLTK data packages (e.g., `punkt`, `punkt_tab`) are downloaded explicitly before using specific NLTK functions, as some functions may have implicit dependencies not covered by general downloads.\n",
        "*   This foundational process can be extended to more advanced NLP tasks such as tokenization, stemming, lemmatization, or part-of-speech tagging, leveraging the installed libraries.\n"
      ]
    }
  ]
}